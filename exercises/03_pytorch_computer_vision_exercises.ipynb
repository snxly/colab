{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snxly/colab/blob/master/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).\n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "6c0d9c4e-b6d5-4483-8f0c-64a2b1272592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else 'cpu'\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "a82f85de-1824-470c-b6bf-a383c8e2fda0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. classification\n",
        "# 2. object detection\n",
        "# 3. category detection"
      ],
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model matches perfect on training set of data, but not so good on testing set of data."
      ],
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# regularization like dropout\n",
        "# change a model\n",
        "# more data"
      ],
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_data = datasets.MNIST(root='MNIST', train=True, transform=ToTensor(), download=True, target_transform=None)\n",
        "test_data = datasets.MNIST(root='MNIST', train=False, transform=ToTensor(), download=True, target_transform=None)\n",
        "\n",
        "print(len(train_data), len(test_data))"
      ],
      "metadata": {
        "id": "SHjeuN81bHza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47378faf-e14c-4a2f-d4c5-f20c5807bf0a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(5):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  image, label = train_data[i]\n",
        "  print(image.shape, label)\n",
        "  plt.imshow(image.squeeze())\n",
        "  plt.title(label)\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "ec465f38-bfb8-45b3-bc7e-eb831bc2db39"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28]) 5\n",
            "torch.Size([1, 28, 28]) 0\n",
            "torch.Size([1, 28, 28]) 4\n",
            "torch.Size([1, 28, 28]) 1\n",
            "torch.Size([1, 28, 28]) 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "print(len(train_loader), len(test_loader))\n"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5a5dc0-2f01-4807-c135-912e47eca08d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "num_in = 1\n",
        "num_hidden = 10\n",
        "num_out = 10\n",
        "\n",
        "class MyMNISTCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=num_in, out_channels=num_hidden, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=num_hidden, out_channels=num_hidden, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=num_hidden, out_channels=num_hidden, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=num_hidden, out_channels=num_hidden, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.classify = nn.Sequential(\n",
        "        # fix\n",
        "        nn.Flatten(),\n",
        "        # The output of the convolutional layers needs to be flattened before passing to the linear layer.\n",
        "        # The calculation for the input features to the linear layer is based on the output size of the last pooling layer.\n",
        "        # For MNIST (28x28), we keep the size in convnet layer, with kernel=3, stride=1, padding=1, and\n",
        "        # after two max pooling layers with kernel_size=2 and stride=2, the spatial dimensions become (28/2)/2 = 7x7.\n",
        "        # With num_hidden channels, the input features to the linear layer will be num_hidden * 7 * 7.\n",
        "        nn.Linear(in_features=num_hidden * 7 * 7, out_features=num_out)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)\n",
        "    # Flatten the output of the convolutional layers\n",
        "    x = x.view(x.size(0), -1) # Reshape to (batch_size, num_hidden * 7 * 7)\n",
        "    x = self.classify(x)\n",
        "    return x\n",
        "\n",
        "model = MyMNISTCNN()\n",
        "print(model.parameters)"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba44229-faaa-4ee7-906d-dfa493a1823f",
        "collapsed": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.parameters of MyMNISTCNN(\n",
            "  (block_1): Sequential(\n",
            "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block_2): Sequential(\n",
            "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classify): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
            "  )\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "def train_step(model, dataset, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  batch_size = len(dataset)\n",
        "  for batch, (X, y) in enumerate(dataset):\n",
        "    logits = model(X)\n",
        "    loss = loss_fn(logits, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch % 100 == 0:\n",
        "      print(f'batch {batch} / {batch_size}, train_loss = {loss}')\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "# epochs\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch {epoch + 1}')\n",
        "  train_step(model, train_loader, loss_fn, optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "jSo6vVWFbNLD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "3d220af5-6785-4dd8-ea82-2d9630173296"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "batch 0 / 1875, train_loss = 2.3028745651245117\n",
            "batch 100 / 1875, train_loss = 0.20791170001029968\n",
            "batch 200 / 1875, train_loss = 0.06334062665700912\n",
            "batch 300 / 1875, train_loss = 0.20199784636497498\n",
            "batch 400 / 1875, train_loss = 0.13665607571601868\n",
            "batch 500 / 1875, train_loss = 0.08315019309520721\n",
            "batch 600 / 1875, train_loss = 0.026693684980273247\n",
            "batch 700 / 1875, train_loss = 0.1258234977722168\n",
            "batch 800 / 1875, train_loss = 0.09107748419046402\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1415079244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1415079244.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataset, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  X, y = next(iter(test_loader))\n",
        "  # print(X.shape, y.shape)\n",
        "  logits = model(X)\n",
        "  pred = torch.softmax(logits, dim=1).argmax(dim=1)\n",
        "  print(pred)\n",
        "  print(y)\n"
      ],
      "metadata": {
        "id": "_YGgZvSobNxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c45fba7-cfa1-4ae8-f1cc-cf13c5e99053"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7, 2, 1, 0, 4, 1, 4, 4, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
            "        4, 0, 7, 4, 0, 1, 3, 1])\n",
            "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
            "        4, 0, 7, 4, 0, 1, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}